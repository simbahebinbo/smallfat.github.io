# pstore

## 简介
 pstore 产品的全称 **Post Store**，定位是一个分布式Key/Value 存储引擎。主要特性是支持分布式事务，支持多副本，支持强一致性协议，去中心化、可弹性伸缩的分布式存储产品。
 
 pstore设计成一个高度自动化产品，降低使用者的使用难度，提高产品易用性，充分发挥程序自动管理能力。提供工业级ACID能力。

 pstore将底层将数据组织成全局有序的Key-Value对，形成一个KV map，KV map逻辑上按照范围被切分成大量的Key空间，每个Key空间称为Tablet。每个Tablet数据由本地KV存储引擎RocksDB存储。每个Tablet通过Raft协议被复制多份，分布到多个pstore节点上，Tablet副本数量可配置。每个Range默认大小为256M，合理的Tablet大小有利于加速节点故障恢复和扩容，及均衡读写负载。

## 词汇定义
 1. Cluster，部署的一个或者多个pstore服务，组成一个集群
 1. Node，具体的一个pstore进程
 1. key/value，高度抽象的用户数据，简称kv，
 1. tablet，集群中一组连续的已排序的数据。存储在Node中
 1. Replicas,复制集，保证至少有3个节点存储数据，确保数据的安全性。
 1. Raft，基于一致性的高可用，复制集中的副本保持数据的强一致性
 1. 一致性：pstore使用一致性表达ACID和CAP理论，即数据无异常。CAP理论：一致性(Consistency)/可用性(Availability)/分区容错性(PartitionTolerance)满足两个以上。一致性,每个读请求接收最近的写请求或者报错(每个节点读取的数据一致)。可用性，每个请求都会收到应答，不管最近的写是成功或者失败。分区容错性，在网络中断，消息丢失或者延迟，系统可以继续工作。
 1. RocksDB,是一个可嵌入的，持久型的key-value存储,由Facebook基于levelDB开发,RocksDB针对Flash存储进行优化，延迟极小。RocksDB使用LSM存储引擎，纯C++编写。
 1. MVCC（Multi-Version Concurrent Control），即多版本并发控制协议，它的目标是在保证数据一致性的前提下，提供一种高并发的访问性能。
 1. GC,垃圾回收MVCC的多版本数据，以减少存储在磁盘上的数据大小。


## 整体架构
 整体架构如下图：
![架构图](architecture.jpg)
 - 集群中存在主要的两个组件，libps是客户端，处理客户的调用请求，根据请求分发给各个后台服务，pstore是后台的存储进程。
 - 整个集群由平等的多个node组成，多个node之间是平等关系
 - 每个node管理多个tablet，tablet是系统管理的最小单元，node中的数据是有序数据
 - 每个tablet存储多组kv数据，从StartKey 到EndKey 的左闭右开区间。
 - 不同node之间tablet 建立raft组
 - 集群可以随时扩容和缩容，系统自动均衡数据。

## 存储模型

 分布式存储系统对数据分区一般有两种方式：Hash 分区和 Range 分区。Hash 分区对每条数据算一个哈希值，映射到一个逻辑分区上，然后通过另外一层映射将逻辑分区映射到具体的机器上，很多数据库中间件、缓存中间件都是这样做的。这种方式的优点是数据写入一般不会出现热点，缺点是原本连续的数据经过 Hash 后散落在不同的分区上变成了无序的，那么，在需要扫描一个范围的数据，需要把所有的分区都扫描一遍，对于节点的扩容和缩容，需要移动部分数据，Hahs方式对于系统的伸缩并不友好。

 相比而言，Range 分区对数据进行范围分区，连续的数据是存储在一起的，可以按需对相邻的分区进行合并，和拆分。大多数的用户在使用的时候可以不关心数据的分布清空，系统会自动进行管理，业界典型的系统是 HBase。这种分区方式的缺点是一、对于追加有序数据，不友好，因为请求都会打到最后一个分片，使得最后一个分片成为瓶颈。优点是更容易处理热点问题，当一个分区过热的时候，可以切分开，迁移到其他的空闲机器上。另外 Range 分区能够支持更丰富的访问模式，使用起来更加灵活。

 和目前大多数存储系统一样，我们也采用 RocksDB 作为单机存储引擎。RocksDB 作为一个通用的存储引擎，提供了不错的性能和稳定性。RocksDB 除了提供基础的读写接口以外，还提供了丰富的选项和功能，以满足各种各样的业务场景。然而在实际生产实践中，要把 RocksDB 用好也不是一件简单的事情。RocksDB 使用LSM的存储模型(详见[LSM介绍](lsm.md))。

## rocksdb
RocksDB项目起源于Facebook的一个实验项目,该项目旨在开发一个与快速存储器（尤其是闪存）存储数据性能相当的数据库软件，以应对高负载服务。这是一个c++库,可用于存储键和值,可以是任意大小的字节流。它支持原子读和写。RocksDB具有高度灵活的配置功能,可以通过配置使其运行在各种各样的生产环境,包括纯内存,Flash,硬盘或HDFS。它支持各种压缩算法，并提供了便捷的生产环境维护和调试工具。(详见[Rocksdb介绍](rocksdb.md))


## Raft强一致性
在分布式系统中,一致性问题(consensus problem)是指对于一组服务器，给定一组操作，我们需要一个协议使得最后它们的结果达成一致。由于CAP理论告诉我们对于分布式系统，如果不想牺牲一致性，我们就只能放弃可用性，所以，数据一致性模型主要有以下几种：强一致性、弱一致性和最终一致性等，在本篇章中，我们主要讨论的算法Raft，是一种分布式系统中的强一致性的实现算法。

强一致性的一般实现的原理：当其中某个服务器收到客户端的一组指令时,它必须与其它服务器交流以保证所有的服务器都是以同样的顺序收到同样的指令,这样的话所有的服务器会产生一致的结果,看起来就像是一台机器一样.

在Raft被提出来之前，Paxos协议是第一个被证明的一致性算法，但是Paxos的论文非常难懂，导致基于Paxos的工程实践和教学都十分头疼，于是Raft在设计的过程中，就从可理解性出发，使用算法分解和减少状态等手段，目前已经应用非常广泛。(详见[Raft介绍](raft.md))

## 元数据

## 事务模型

pstore是一个支持完整事务ACID的分布式KV存储引擎。它支持隔离级别是Serializable Snapshot。简称SSI，pstore实现了一个无锁的乐观事务模型，事务冲突通过事务重启或者回滚尽快返回客户端，然后由客户端决策下一步如何处理。pstore的事务模型参考了Google的Percolator模型，Percolator是Google设计的分布式事务协议，借助Bigtable原生的单行事务实现了跨行事务。Percolator的设计理念集中体现在OSDI 2010的一篇论文《Large-scale Incremental Processing Using Distributed Transactions and Notifications》中。


## HLC

全局时钟是分布式事务的基石，pstore使用HLC算法提供时钟。HLC由WallTime和LogicTime两部分组成。WallTime为节点n当前已知的最大的物理时间，通过先判断WallTime，再判断LogicTime确定两个事件的先后顺序。

在给本地节点产生的事件分配HLC时间时，WallTime部分取当前WallTime和当前物理时间最大值。如果物理时间小于或等于WallTime，LogicTime在原有基础上加一；如果物理时间大于WallTime，LogicTime归零。

节点之间的消息交换都会附带上消息产生时获取的HLC时间，当任一节点收到其他节点发送过来的消息时，取当前节点的WallTime、对端HLC时间的WallTime以及本地物理时间中的最大值。若三者相等，则取当前节点的LogicTime和对端LogicTime最大值加一；若对端WallTime最大，则取对端LogicTime加一；若本地WallTime最大，则取本地LogicTime加一。新的HLC时间更新到本地并作为本地下一个本地事件使用的HLC时间。

总而言之，WallTime表示事件发生时，当前节点所能感知到的最大物理时间，而LogicTime表示当WallTime相同时，两个事件的顺序。HLC算法可以保证如下特性：

1. 对于事件A，B，如果满足HLC.A < HLC.B，那么事件A一定发生在事件B之前，即A happen before B。

2. WallTime一定满足WallTime ≥ Node.pt。Node.pt即节点的本地物理时间。HLC只会一直增大，不会因为物理时间的波动而回退。

3. 如果发现WallTime > Node.pt，那么一定存在一个已经发生的事件X把当前节点的HLC往前推进了。

4. WallTime-Node.pt是有界的，它一定小于一个值ε。对于任意两个事件A和B，如果A happen before B，那么一定存在B.pt + ε ≥  A.pt。后面我们可以看到这个收敛的偏差ε的大小如何影响事务的冲突决策。

5. HLC可以满足全局快照的需求。这是因为当以一个HLC作为快照时间点时，这个时间会导致所有节点的HLC向前推进，因此这个时间之后发生的事件对于这个快照一定不可见，进而保证快照的读取安全有效。

## MVCC和Write Intent

底层存储是KV，因此利用KEY + timestamp即可实现MVCC。在实际的操作中，为了快速读取到最新的数据，会把HLC取反encode到KEY的尾部，读取的时候以KEY作为前缀启动一个迭代器，这样最新的数据最先被读取到。

事务中未提交的数据比较特殊，并不会把事务时间戳encode到KEY的尾部，而是把事务相关的信息和数据一起Encode到Value中，称之为WRITE INTENT。读取的时候，未提交的数据肯定最先被Get到。

数据的存储存在两种情况：
- Key TxnMeta，这种情况只有key，value部分是事务的信息，该种数据称外Write Intent，代表数据正在被写入
- Key+version value，这种带有时间戳版本的数据是用户数据，可能存在key相同，时间戳不同的多个版本的数据，即MVCC，过期的历史数据会被异步GC掉。

为所有事务的写入write Intents，表示临时的，未提交的状态。 它们与标准多版本并发控制（MVCC）值基本相同，但也包含指向存储在集群上的事务记录的指针。

## 事务隔离

默认使用SSI隔离级别。在对性能要求较高。在冲突较少的情况下，SSI不需要加锁或额外写操作。在冲突激烈的情况下，SSI仍然不需要加锁，但是会有更多事务被终止。SSI 是默认的级别，见 Cahill 的论文。这是另一篇伟大的论文。 关于通过避免读-写冲突（相对于检测它们，称之为写-快照隔离）实现 SSI 的讨论。

## 事务记录

pstore通过一张分布式的全局事务表记录事务记录。tablet中的数据分为系统数据和用户数据（通过不同的前缀区分）。一个事务的事务记录一般存储在该事务的第一条写记录所在的tablet，事务记录作为系统数据而存储和用户数据区分开来。对于只读的事务可以不存储事务记录，对于写事务在服务端存储了事务记录，事务记录分布在第一个写语句key所在的tablet上，写事务存在心跳信息。

一个事务记录包含如下核心信息：
- UUID：事务记录唯一标识符
- Key：事务记录的Key，用来定位事务记录的位置
- ReadTimestamp：事务开始时间
- WriteTimestamp：事务提交的候选时间，在一开始的时候和ReadTimestamp相同。
- Status：事务状态，PENDING、COMMITED、ABORTED
- Priority：事务优先级
- LastHeartbeat：事务协调者发送的最后一次心跳时间

事务开始时，事务创建者会创建一个唯一的事务ID（UUID），同时取一个时间戳作为事务的后备提交时间戳，这个时间戳可能会因为事务冲突而被修改。随机分配一个事务优先级，这个优先级在将来的事务冲突裁决时有用，事务的优先级也会因为事务的重启而发生变化。

事务的初始状态时PENDING，事务过程中所有的写都被以INTENT的方式写入tablet，当参与事务的记录都prepare success之后，协调者只需要修改事务记录的状态为COMMITED即可返回给客户端，单条记录的修改raft和RocksDB均可以保证，进而保证事务的原子性。协调者会将事务数据异步提交，但是无需保证一定提交（事务冲突章节会详细描述解决之道）。

## 两阶段事务

 系统实现的是一个无锁的两阶段提交事务模型，事务冲突通过事务重启或者回滚尽快返回客户端由客户端决策下一步如何处理。两阶段事务具体执行过程如下所示：

1. 产生事务记录，事务状态为PENDING，也就是BeginTransactoin。
2. 参与节点以WRITE INTENT的形式写入数据，并返回候选时间戳。
3. 比较候选时间戳中最大的时间戳和事务起始时间戳是否相等，以及事务隔离级别，决定事务状态被修改为COMMITED还是ABORTED。如果候选和提交时间戳不相等则事务重启。
4. 事务提交/回滚之后，残留的WRITE INTENT将被异步清理。
5. 通常情况下会选择事务中遇到的第一个写操作的Key作为事务记录的Key，此时才会真正把事务记录持久化到事务记录表中。这样做的好处是，对于只读事务不需要记录事务状态。

## 一阶段事务

如果一个事务所有的写提交都落在一个tablet上，那么系统会启动一个Fast 1PC，将所有的修改记录一次性提交给tablet，由raft log保证这些记录的ACID，免去写事务记录和INTENT。

## 事务冲突

当读写操作遇到Intent 记录或新提交的数据时就产生了事务冲突。事务冲突会导致事务重启或者事务中断。

    1. 事务重启，会分配一个新的优先级和更大的HLC，并且复用原来的事务ID，事务之前曾经写入一些Intent，这些需要显式的清理，不过一般情况下，事务重启会覆盖原来的记录，因此并不需要这样做。

    1. 事务中断，事务记录中的状态时Abort，这时需要将控制权交给客户端，然后删除事务已经写入的Intent，协调者并不保证一定删除这些Intent，后续的其他读写事务遇到冲突的时候会顺便删除这些记录。

### 读写冲突

当一个读请求（时间戳Read.t,优先级Read.p）读取到INTENT记录后，会比较INTENT中的事物提交时间戳(Txn.t)和优先级(Txn.p)裁定冲突解决。

    如果Read.t < Txn.t，分两种情况：

    A.  Read.t + ε< Txn.t，没有冲突，读操作可以继续。如果读操作遇到本事务内的一个Intent，那么这个Intent可以读。
    B.  Read.t < Txn.t < Read.t + ε，这种场景下不能确认Intent是否对读事务可见，因此读事务以更大的时间戳重启。


    如果Read.t > Txn.t，查询写事务的事务记录，如果事务已经提交，那么Intent可见，否则，分两种情况处理

    A.   如果该Intent来自于SI隔离级别的事务，则把该写事务的提交时间戳推迟（因此该Intent对读事务不可见）。实现很简单，只需更新写事务在事务记录表中的事务提交时间戳，从而保证该事务必定使用一个更大的时间戳提交。

    B.   如果该Intent来自于SSI隔离级别的事务，则先比较两者优先级。如果读事务具有更高优先级，则延后写事务的提交时间戳（该写事务提交时发现提交时间戳被修改，则事务重启）；如果优先级相同或更低，则读事务使用新优先级max(新随机优先级，写事务优先级-1)重启。

### 写读冲突

Range上都维护一个timestamp cache，读操作会更新这个cache，写事务结束时应用这个cache。

每个写操作都会访问所在节点的读缓存（Read Timestamp Cache）。如果写操作的候选时间戳早于缓存的低水位线（最后被换出的时间戳），那么读缓存的低水位线将作为该事务新的候选时间戳；如果写操作遇到一条Key相同且读时间戳晚于当前事务候选时间戳的记录，那么Key的读时间戳将作为该事务新的候选时间戳。


### 写写冲突与Push

如果写操作遇到未被提交的Intent：
    1. 如果写入Intent的事务优先级更低，则该事务被终止；
    2. 如果写入Intent的事务优先级相同或更高，则该事务随机等待一段时间后，以新优先级max(随机优先级，当前事务优先级-1)重启。

如果写操作遇到新提交数据：无论该已提交的数据是Intent或非Intent，当前事务以相同优先级重启，而且使用该数据时间戳作为候选时间戳。

### 冲突流程图

![图1](txn.jpg)


### 事务心跳

事务协调者在创建事务记录之后会定期向事务记录发送心跳，即修改事务记录的LastHeartBeat。如果事务协调者异常，那么最多一个心跳周期后，其他冲突的事务就会检测到这种异常，然后确认是提交事务还是终止事务。



 